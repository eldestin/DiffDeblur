{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from basicsr.archs.pwcnet_arch import FlowGenerator\n",
    "from einops import rearrange\n",
    "from DAM import DAModule\n",
    "import math\n",
    "\n",
    "class Cross_attention(nn.Module):\n",
    "    def __init__(self, dim_q, dim_k, dim_v, dim ,num_heads, bias):\n",
    "        super(Cross_attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        self.wq = nn.Conv2d(dim_q, dim, kernel_size=1, bias=bias)\n",
    "        self.wk = nn.Conv2d(dim_k, dim, kernel_size=1, bias=bias)\n",
    "        self.wv = nn.Conv2d(dim_v, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        b,c,h,w = q.shape\n",
    "\n",
    "        q, k, v = self.wq(q), self.wk(k), self.wv(v) \n",
    "        # print(q.shape)\n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        \n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "        # print(\"Rearrange q shape: \", q.shape)\n",
    "        # print(\"Rearrange k shape: \", k.shape)\n",
    "        # print(\"Rearrange v shape: \", v.shape)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ v)\n",
    "        \n",
    "        print(\"out\",out.shape)\n",
    "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "\n",
    "        out = self.project_out(out)\n",
    "        return out\n",
    "\n",
    "class single_conv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),\n",
    "                                  nn.LeakyReLU(negative_slope=0.2, inplace=True),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),\n",
    "                                  nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                  nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),\n",
    "                                  nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class double_conv_down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, stride=2, padding=1),\n",
    "                                  nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                  nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),\n",
    "                                  nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class double_conv_up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv_up, self).__init__()\n",
    "        self.conv = nn.Sequential(nn.UpsamplingNearest2d(scale_factor=2),\n",
    "                                  nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),\n",
    "                                  nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                  nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),\n",
    "                                  nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, in_ch, feat_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv_in = single_conv(in_ch, feat_ch)\n",
    "\n",
    "        self.conv1 = double_conv_down(feat_ch, feat_ch)\n",
    "        self.conv2 = double_conv_down(feat_ch, feat_ch)\n",
    "        self.conv3 = double_conv(feat_ch, feat_ch)\n",
    "        self.conv4 = double_conv_up(feat_ch, feat_ch)\n",
    "        self.conv5 = double_conv_up(feat_ch, feat_ch)\n",
    "        self.conv6 = double_conv(feat_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat0 = self.conv_in(x)    # H, W\n",
    "        feat1 = self.conv1(feat0)   # H/2, W/2\n",
    "        feat2 = self.conv2(feat1)    # H/4, W/4\n",
    "        feat3 = self.conv3(feat2)    # H/4, W/4\n",
    "        feat3 = feat3 + feat2     # H/4\n",
    "        feat4 = self.conv4(feat3)    # H/2, W/2\n",
    "        feat4 = feat4 + feat1    # H/2, W/2\n",
    "        feat5 = self.conv5(feat4)   # H\n",
    "        feat5 = feat5 + feat0   # H\n",
    "        feat6 = self.conv6(feat5)\n",
    "\n",
    "        return feat0, feat1, feat2, feat3, feat4, feat6\n",
    "\n",
    "class PosEnSine(nn.Module):\n",
    "    \"\"\"\n",
    "    Code borrowed from DETR: models/positional_encoding.py\n",
    "    output size: b*(2.num_pos_feats)*h*w\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_pos_feats):\n",
    "        super(PosEnSine, self).__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.normalize = True\n",
    "        self.scale = 2 * math.pi\n",
    "        self.temperature = 10000\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        not_mask = torch.ones(1, h, w, device=x.device)\n",
    "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
    "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
    "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
    "\n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        pos = pos.repeat(b, 1, 1, 1)\n",
    "        return pos\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    conv-based MLP layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        self.linear1 = nn.Conv2d(in_features, hidden_features, 1)\n",
    "        self.act = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        self.linear2 = nn.Conv2d(hidden_features, out_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class Transformer_enc(nn.Module):\n",
    "    def __init__(self, feature_dims, n_head = 4, mlp_ratio = 2, depth_cat = False):\n",
    "        super(Transformer_enc, self).__init__()\n",
    "        dim_q, dim_k, dim_v, feature_dim = feature_dims\n",
    "        self.pos_enc = PosEnSine(feature_dim//2)\n",
    "        if depth_cat:\n",
    "            self.pos_enc = PosEnSine(feature_dim)\n",
    "        self.att = Cross_attention(dim_q, dim_k, dim_v, feature_dim, n_head, False)\n",
    "        mlp_hidden = int(feature_dim * mlp_ratio)\n",
    "        self.FFN = MLP(feature_dim, mlp_hidden)\n",
    "        self.norm = nn.GroupNorm(1,  feature_dim)\n",
    "        self.res = double_conv(dim_q, feature_dim)\n",
    "    def forward(self, q, k, v):\n",
    "        # print(q.shape)\n",
    "        b,c,h,w = q.shape\n",
    "        q_pos = self.pos_enc(q)\n",
    "        k_pos = self.pos_enc(k)\n",
    "        # print(q_pos.shape)\n",
    "        if c > 1:\n",
    "            att_out = self.att(q + q_pos, k + k_pos, v)\n",
    "        else:\n",
    "            att_out = self.att(q, k + k_pos, v)\n",
    "        # print(\"Attention output shape: \", att_out.shape)\n",
    "        first_res = self.res(q) + att_out\n",
    "        # pass MLP\n",
    "        mlp_out = self.FFN(first_res)\n",
    "        second_res = mlp_out + first_res\n",
    "        out = self.norm(second_res)\n",
    "        return out\n",
    "\n",
    "class Alignformer(nn.Module):\n",
    "    '''\n",
    "       The implementation of utilizing depth map to guide attention (concat/cross attention)\n",
    "    '''\n",
    "    def __init__(self, feature_dims, src_channel, ref_channel, \n",
    "                    out_channel, n_head = 4, mlp_ratio = 2, depth_cat = False):\n",
    "        super(Alignformer, self).__init__()\n",
    "        feature_dim = feature_dims[-1]\n",
    "        # print(feature_dim)\n",
    "        self.DAM = DAModule(in_ch = src_channel, feat_ch = feature_dim, out_ch = src_channel,\n",
    "                            demodulate = True, requires_grad = True)\n",
    "        # Define feature extractor\n",
    "        self.unet_q = Unet(src_channel, feature_dim, feature_dim)\n",
    "        self.unet_k = Unet(src_channel, feature_dim, feature_dim)\n",
    "        # Define GAM\n",
    "        self.trans_unit = nn.ModuleList([\n",
    "            Transformer_enc(feature_dims, n_head, mlp_ratio,depth_cat = depth_cat),\n",
    "            Transformer_enc(feature_dims, n_head, mlp_ratio, depth_cat = depth_cat),\n",
    "            Transformer_enc(feature_dims, n_head, mlp_ratio, depth_cat = depth_cat)\n",
    "        ])\n",
    "        # Unet result output\n",
    "        self.conv0 = double_conv(feature_dim, feature_dim)\n",
    "        self.conv1 = double_conv_down(feature_dim, feature_dim)\n",
    "        self.conv2 = double_conv_down(feature_dim, feature_dim)\n",
    "        self.conv3 = double_conv(feature_dim, feature_dim)\n",
    "        self.conv4 = double_conv_up(feature_dim, feature_dim)\n",
    "        self.conv5 = double_conv_up(feature_dim, feature_dim)\n",
    "        self.conv6 = nn.Sequential(single_conv(feature_dim, feature_dim), \n",
    "                        nn.Conv2d(feature_dim, out_channel, 3, 1, 1))\n",
    "        \n",
    "    def forward(self, x_l, x_r, dpt_map):\n",
    "        l_dpt, r_dpt = dpt_map\n",
    "        x_l, x_r = torch.cat([x_l, l_dpt], dim = 1), torch.cat([x_r, r_dpt], dim = 1)\n",
    "        src = self.DAM(x_l, x_r)\n",
    "        q_feature = self.unet_q(x_l)\n",
    "        k_feature = self.unet_k(x_r)\n",
    "        outputs = []\n",
    "    \n",
    "        for i in range(3):\n",
    "            # print(\"Query feature shape:\",q_feature[i+3].shape)\n",
    "            # print(\"Depth feature shape:\",depth_feature_l[i+3].shape)\n",
    "            # print(\"Key feature shape:\",k_feature[i+3].shape)\n",
    "            outputs.append(self.trans_unit[i](\n",
    "                q_feature[i+3], \n",
    "                k_feature[i+3], k_feature[i+3]\n",
    "            ))\n",
    "        f0 = self.conv0(outputs[2])  # H, W\n",
    "        f1 = self.conv1(f0)  # H/2, W/2\n",
    "        f1 = f1 + outputs[1]\n",
    "        f2 = self.conv2(f1)  # H/4, W/4\n",
    "        f2 = f2 + outputs[0]\n",
    "        f3 = self.conv3(f2)  # H/4, W/4\n",
    "        f3 = f3 + outputs[0] + f2\n",
    "        f4 = self.conv4(f3)   # H/2, W/2\n",
    "        f4 = f4 + outputs[1] + f1\n",
    "        f5 = self.conv5(f4)   # H, W\n",
    "        f5 = f5 + outputs[2] + f0\n",
    "        out = self.conv6(f5)\n",
    "\n",
    "        return out\n",
    "class Alignformer_depMapG(nn.Module):\n",
    "    '''\n",
    "       The implementation of utilizing depth map to guide attention (concat/cross attention)\n",
    "    '''\n",
    "    def __init__(self, feature_dims, src_channel, ref_channel, \n",
    "                    out_channel, n_head = 4, mlp_ratio = 2, depth_cat = False, module = \"cat\"):\n",
    "        super(Alignformer_depMapG, self).__init__()\n",
    "        feature_dim = feature_dims[-1]\n",
    "        # print(feature_dim)\n",
    "        self.DAM = DAModule(in_ch = src_channel, feat_ch = feature_dim, out_ch = src_channel,\n",
    "                            demodulate = True, requires_grad = True)\n",
    "        # Define feature extractor\n",
    "        self.unet_q = Unet(src_channel, feature_dim, feature_dim)\n",
    "        self.unet_k = Unet(src_channel, feature_dim, feature_dim)\n",
    "        self.unet_d = Unet(1, feature_dim, feature_dim)\n",
    "        # Define GAM\n",
    "        self.trans_unit = nn.ModuleList([\n",
    "            Transformer_enc(feature_dims, n_head, mlp_ratio,depth_cat = depth_cat),\n",
    "            Transformer_enc(feature_dims, n_head, mlp_ratio, depth_cat = depth_cat),\n",
    "            Transformer_enc(feature_dims, n_head, mlp_ratio, depth_cat = depth_cat)\n",
    "        ])\n",
    "        self.guide_module = module\n",
    "        # Unet result output\n",
    "        self.conv0 = double_conv(feature_dim, feature_dim)\n",
    "        self.conv1 = double_conv_down(feature_dim, feature_dim)\n",
    "        self.conv2 = double_conv_down(feature_dim, feature_dim)\n",
    "        self.conv3 = double_conv(feature_dim, feature_dim)\n",
    "        self.conv4 = double_conv_up(feature_dim, feature_dim)\n",
    "        self.conv5 = double_conv_up(feature_dim, feature_dim)\n",
    "        self.conv6 = nn.Sequential(single_conv(feature_dim, feature_dim), \n",
    "                        nn.Conv2d(feature_dim, out_channel, 3, 1, 1))\n",
    "        \n",
    "    def forward(self, x_l, x_r, dpt_map):\n",
    "        l_dpt, r_dpt = dpt_map\n",
    "        src = self.DAM(x_l, x_r)\n",
    "        q_feature = self.unet_q(x_l)\n",
    "        k_feature = self.unet_k(x_r)\n",
    "        depth_feature_l, depth_feature_r = self.unet_d(l_dpt), self.unet_d(r_dpt)\n",
    "        outputs = []\n",
    "        if self.guide_module == \"cat\":\n",
    "            for i in range(3):\n",
    "                # print(\"Query feature shape:\",q_feature[i+3].shape)\n",
    "                # print(\"Depth feature shape:\",depth_feature_l[i+3].shape)\n",
    "                # print(\"Key feature shape:\",k_feature[i+3].shape)\n",
    "                outputs.append(self.trans_unit[i](\n",
    "                    torch.cat([q_feature[i+3], depth_feature_l[i+3]], dim = 1), \n",
    "                    torch.cat([k_feature[i+3], depth_feature_r[i+3]], dim = 1), k_feature[i+3]\n",
    "                ))\n",
    "        f0 = self.conv0(outputs[2])  # H, W\n",
    "        f1 = self.conv1(f0)  # H/2, W/2\n",
    "        f1 = f1 + outputs[1]\n",
    "        f2 = self.conv2(f1)  # H/4, W/4\n",
    "        f2 = f2 + outputs[0]\n",
    "        f3 = self.conv3(f2)  # H/4, W/4\n",
    "        f3 = f3 + outputs[0] + f2\n",
    "        f4 = self.conv4(f3)   # H/2, W/2\n",
    "        f4 = f4 + outputs[1] + f1\n",
    "        f5 = self.conv5(f4)   # H, W\n",
    "        f5 = f5 + outputs[2] + f0\n",
    "        out = self.conv6(f5)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained depth model: \n",
      "crop size 320\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import OrderedDict\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "from utils.Haze4k import DPDD, LFDOF\n",
    "from Segmodels.segformer_mit import SegFormer\n",
    "from torch.utils.data import DataLoader\n",
    "def convert_pl(path):\n",
    "    '''\n",
    "    This function aims to convert PT lightning parameters dictionary into torch load state dict \n",
    "    '''\n",
    "    ckpt = torch.load(path,map_location='cpu')\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k in ckpt['state_dict']:\n",
    "        # print(k)\n",
    "        #name = 1\n",
    "        # print(k[:4])\n",
    "        if k[:4] != 'net.':\n",
    "        #if 'tiny_unet.' not in k:\n",
    "            continue\n",
    "        name = k.replace('net.','')\n",
    "        # print(name)\n",
    "        new_state_dict[name] = ckpt['state_dict'][k]\n",
    "    return new_state_dict\n",
    "depth_model = SegFormer()\n",
    "depth_model_syn = SegFormer()\n",
    "depth_model_real = SegFormer()\n",
    "depth_path_syn = \"/home/fhx/code/archive/models/DEM_Segformer/logs/Seg_Logs/synthetic/checkpoints/Seg-epoch=94-psnr=22.814829-ssim=0.843821.ckpt\"\n",
    "depth_path_real = \"/home/fhx/code/archive/models/DEM_Segformer/logs/Seg_Logs/version_0/checkpoints/Seg-epoch=94-iou=0.884012.ckpt\"\n",
    "# checkpoint = torch.load(self.depth_path)\n",
    "#print(checkpoint[\"state_dict\"])\n",
    "ckpt_syn = convert_pl(depth_path_syn)\n",
    "ckpt_real = convert_pl(depth_path_real)\n",
    "print(\"Load pretrained depth model: \")\n",
    "depth_model_syn.load_state_dict(ckpt_syn)\n",
    "depth_model_real.load_state_dict(ckpt_real)\n",
    "val_set = DPDD('/home/fhx/Datasets/DPDD_dataset/train 1/train',train=False,size=320,crop=False, name = False)\n",
    "val_loader = DataLoader(val_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "# for i, j in enumerate(val_loader):\n",
    "#     blur, clear, name = j\n",
    "#     clear_map = depth_model_real(clear)\n",
    "#     print(name)\n",
    "#     clear_name = \"save_img/\" + name[0]\n",
    "#     clear_map = (clear_map - clear_map.min()) / (clear_map.max() - clear_map.min())\n",
    "#     save_image(clear_map,clear_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1P0A2488.png',)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur, gt, depth = next(iter(val_loader))\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "save_image(depth,\"depth.png\")\n",
    "\n",
    "# depth_map = depth_model(blur)\n",
    "# depth_map_syn = depth_model_syn(blur)\n",
    "# depth_map_real = depth_model_real(blur)\n",
    "# depth_map_gt = depth_model(gt)\n",
    "# depth_map_syn_gt = depth_model_syn(gt)\n",
    "# depth_map_real_gt = depth_model_real(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.utils as tv_utils\n",
    "\n",
    "# Assuming 'defocus_map' is your actual defocus map tensor\n",
    "# Normalize to [0, 1]\n",
    "defocus_map_normalized = (depth_map_real - depth_map_real.min()) / (depth_map_real.max() - depth_map_real.min())\n",
    "defocus_syn_normalized = (depth_map_syn - depth_map_syn.min()) / (depth_map_syn.max() - depth_map_syn.min())\n",
    "defocus = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())\n",
    "\n",
    "\n",
    "defocus_map_normalized_gt = (depth_map_real_gt - depth_map_real_gt.min()) / (depth_map_real_gt.max() - depth_map_real_gt.min())\n",
    "defocus_syn_normalized_gt = (depth_map_syn_gt - depth_map_syn_gt.min()) / (depth_map_syn_gt.max() - depth_map_syn_gt.min())\n",
    "defocus_gt = (depth_map_gt - depth_map_gt.min()) / (depth_map_gt.max() - depth_map_gt.min())\n",
    "# Convert to 8-bit integer (0-255)\n",
    "# defocus_map_uint8 = (defocus_map_normalized * 255).clamp(0, 255).to(torch.uint8)\n",
    "\n",
    "# Save as PNG\n",
    "# save_path = \"defocus_map_normalized.png\"\n",
    "# tv_utils.save_image(defocus_map_normalized, save_path)\n",
    "\n",
    "# print(f\"Normalized defocus map saved as {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "save_image(blur,\"blur.png\")\n",
    "save_image(defocus,\"hg_depth_blur.png\")\n",
    "save_image(defocus_syn_normalized,\"syn_depth_blur.png\")\n",
    "save_image(defocus_map_normalized,\"real_depth_blur.png\")\n",
    "save_image(gt,\"gt.png\")\n",
    "save_image(defocus_gt,\"hg_depth.png\")\n",
    "save_image(defocus_syn_normalized_gt,\"syn_depth.png\")\n",
    "save_image(defocus_map_normalized_gt,\"real_depth.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 3, 3]), torch.Size([2, 1, 3, 3]), torch.Size([2, 3, 3, 3]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((2, 7, 3,3))\n",
    "a[:, :3, :, :].shape, a[:, 3:4, :, :].shape, a[:,4:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0548, -0.0567,  0.9441],\n",
       "          [-0.6711, -1.3959, -0.2184],\n",
       "          [ 2.3578,  1.6532, -0.8417]],\n",
       "\n",
       "         [[ 0.6245, -0.4252,  1.0264],\n",
       "          [ 0.1983,  2.0613,  1.7486],\n",
       "          [ 0.2571,  0.9472,  0.7934]],\n",
       "\n",
       "         [[ 0.9532,  1.5996,  0.5311],\n",
       "          [ 0.6595,  2.5955, -0.4356],\n",
       "          [-0.3221,  0.3520,  0.2600]]],\n",
       "\n",
       "\n",
       "        [[[-0.9736,  0.0792, -1.1424],\n",
       "          [ 0.9019, -1.0491, -0.3803],\n",
       "          [ 1.7257, -0.6090,  1.9963]],\n",
       "\n",
       "         [[ 0.5129,  1.2238, -0.3489],\n",
       "          [-1.1229, -0.5799,  0.0894],\n",
       "          [ 0.2790,  0.4487,  0.8890]],\n",
       "\n",
       "         [[-0.5826,  0.1090, -1.0912],\n",
       "          [-2.0799,  2.0098,  1.7442],\n",
       "          [ 0.8457,  2.0026, -0.5016]]]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, :3, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out torch.Size([2, 4, 8, 3136])\n",
      "out torch.Size([2, 4, 8, 12544])\n",
      "out torch.Size([2, 4, 8, 50176])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test result\n",
    "dpt_map = [torch.randn(2,1,224,224), torch.randn(2,1,224,224)]\n",
    "l,r = torch.randn(2, 3, 224, 224), torch.randn(2, 3, 224, 224)\n",
    "model = Alignformer_depMapG([64, 64, 32, 32], 3,3,3, depth_cat = True)\n",
    "model(l,r,dpt_map).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_attention(nn.Module):\n",
    "    def __init__(self, dim_q, dim_k, dim_v, dim ,num_heads, bias):\n",
    "        super(Cross_attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        self.wq = nn.Conv2d(dim_q, dim, kernel_size=1, bias=bias)\n",
    "        self.wk = nn.Conv2d(dim_k, dim, kernel_size=1, bias=bias)\n",
    "        self.wv = nn.Conv2d(dim_v, dim, kernel_size=1, bias=bias)\n",
    "        self.wshare = nn.Conv2d(dim, dim, kernel_size=3,  stride=1, padding=1,groups=dim,bias=bias)\n",
    "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        b,c,h,w = q.shape\n",
    "\n",
    "        q, k, v = self.wshare(self.wq(q)), self.wshare(self.wk(k)), self.wshare(self.wv(v)) \n",
    "        print(q.shape)\n",
    "        # print(q.shape)\n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        \n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "        # print(\"Rearrange q shape: \", q.shape)\n",
    "        # print(\"Rearrange k shape: \", k.shape)\n",
    "        # print(\"Rearrange v shape: \", v.shape)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ v)\n",
    "        \n",
    "        print(\"out\",out.shape)\n",
    "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "\n",
    "        out = self.project_out(out)\n",
    "        return out\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, bias):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        self.qkv = nn.Conv2d(dim, dim*3, kernel_size=1, bias=bias)\n",
    "        self.qkv_dwconv = nn.Conv2d(dim*3, dim*3, kernel_size=3, stride=1, padding=1, groups=dim*3, bias=bias)\n",
    "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.shape\n",
    "\n",
    "        qkv = self.qkv_dwconv(self.qkv(x))\n",
    "        print(qkv.shape)\n",
    "        q,k,v = qkv.chunk(3, dim=1)   \n",
    "        # print(q.shape)\n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ v)\n",
    "        print(out.shape)\n",
    "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "        \n",
    "        out = self.project_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 128, 128])\n",
      "out torch.Size([2, 8, 8, 16384])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 128, 128])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = Cross_attention(64,64,64,64,8,False)\n",
    "tst = torch.randn((2,64,128,128))\n",
    "att(tst,tst,tst).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 192, 128, 128])\n",
      "torch.Size([2, 8, 8, 16384])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 128, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att2 = Attention(64,8,True)\n",
    "att2(tst).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTransformer_enc_guide\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m        The implementation of guidance utilizing depth map (cross attention)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, feature_dims, n_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, mlp_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, depth_cat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Transformer_enc_guide(nn.Module):\n",
    "    '''\n",
    "        The implementation of guidance utilizing depth map (cross attention)\n",
    "    '''\n",
    "    def __init__(self, feature_dims, n_head = 4, mlp_ratio = 2, depth_cat = False):\n",
    "        super(Transformer_enc_guide, self).__init__()\n",
    "        self.tf_block_l = Transformer_enc(feature_dims, n_head, mlp_ratio, depth_cat)\n",
    "        self.tf_block_r = Transformer_enc(feature_dims, n_head, mlp_ratio, depth_cat)\n",
    "        self.tf_block_all = Transformer_enc(feature_dims, n_head, mlp_ratio, depth_cat)\n",
    "    \n",
    "    def forward(self, x_l, x_r, dpt_map):\n",
    "        # block_1 \n",
    "        dpt_l, dpt_r = dpt_map\n",
    "        att_l_out = self.tf_block_l(dpt_l, x_l, x_l)\n",
    "        att_r_out = self.tf_block_r(dpt_r, x_r, x_r)\n",
    "        att_out = self.tf_block_all(att_l_out, att_r_out, att_r_out)\n",
    "        return att_out\n",
    "\n",
    "\n",
    "class Alignformer_depMapCross(nn.Module):\n",
    "    '''\n",
    "       The implementation of utilizing depth map to guide attention (concat/cross attention)\n",
    "    '''\n",
    "    def __init__(self, feature_dims, src_channel, ref_channel, \n",
    "                    out_channel, n_head = 4, mlp_ratio = 2, depth_cat = False, module = \"cat\"):\n",
    "        super(Alignformer_depMapCross, self).__init__()\n",
    "        feature_dim = feature_dims[-1]\n",
    "        # print(feature_dim)\n",
    "        self.DAM = DAModule(in_ch = src_channel, feat_ch = feature_dim, out_ch = src_channel,\n",
    "                            demodulate = True, requires_grad = True)\n",
    "        # Define feature extractor\n",
    "        self.unet_q = Unet(src_channel, feature_dim, feature_dim)\n",
    "        self.unet_k = Unet(src_channel, feature_dim, feature_dim)\n",
    "        self.unet_d = Unet(1, feature_dim, feature_dim)\n",
    "        # Define GAM\n",
    "        self.trans_unit = nn.ModuleList([\n",
    "            Transformer_enc_guide(feature_dims, n_head, mlp_ratio,depth_cat = depth_cat),\n",
    "            Transformer_enc_guide(feature_dims, n_head, mlp_ratio, depth_cat = depth_cat),\n",
    "            Transformer_enc_guide(feature_dims, n_head, mlp_ratio, depth_cat = depth_cat)\n",
    "        ])\n",
    "        self.guide_module = module\n",
    "        # Unet result output\n",
    "        self.conv0 = double_conv(feature_dim, feature_dim)\n",
    "        self.conv1 = double_conv_down(feature_dim, feature_dim)\n",
    "        self.conv2 = double_conv_down(feature_dim, feature_dim)\n",
    "        self.conv3 = double_conv(feature_dim, feature_dim)\n",
    "        self.conv4 = double_conv_up(feature_dim, feature_dim)\n",
    "        self.conv5 = double_conv_up(feature_dim, feature_dim)\n",
    "        self.conv6 = nn.Sequential(single_conv(feature_dim, feature_dim), \n",
    "                        nn.Conv2d(feature_dim, out_channel, 3, 1, 1))\n",
    "        \n",
    "    def forward(self, x_l, x_r, dpt_map):\n",
    "        l_dpt, r_dpt = dpt_map\n",
    "        src = self.DAM(x_l, x_r)\n",
    "        q_feature = self.unet_q(x_l)\n",
    "        k_feature = self.unet_k(x_r)\n",
    "        depth_feature_l, depth_feature_r = self.unet_d(l_dpt), self.unet_d(r_dpt)\n",
    "        outputs = []\n",
    "        for i in range(3):\n",
    "            # print(\"Query feature shape:\",q_feature[i+3].shape)\n",
    "            # print(\"Depth feature shape:\",depth_feature_l[i+3].shape)\n",
    "            # print(\"Key feature shape:\",k_feature[i+3].shape)\n",
    "            outputs.append(self.trans_unit[i](\n",
    "                q_feature[i+3], \n",
    "                k_feature[i+3],\n",
    "                [depth_feature_l[i+3], depth_feature_r[i+3]]\n",
    "            ))\n",
    "        f0 = self.conv0(outputs[2])  # H, W\n",
    "        f1 = self.conv1(f0)  # H/2, W/2\n",
    "        f1 = f1 + outputs[1]\n",
    "        f2 = self.conv2(f1)  # H/4, W/4\n",
    "        f2 = f2 + outputs[0]\n",
    "        f3 = self.conv3(f2)  # H/4, W/4\n",
    "        f3 = f3 + outputs[0] + f2\n",
    "        f4 = self.conv4(f3)   # H/2, W/2\n",
    "        f4 = f4 + outputs[1] + f1\n",
    "        f5 = self.conv5(f4)   # H, W\n",
    "        f5 = f5 + outputs[2] + f0\n",
    "        out = self.conv6(f5)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test result\n",
    "dpt_map = [torch.randn(2,1,224,224), torch.randn(2,1,224,224)]\n",
    "l,r = torch.randn(2, 3, 224, 224), torch.randn(2, 3, 224, 224)\n",
    "model = Alignformer_depMapCross([32, 32, 32, 32], 3,3,3, depth_cat = False)\n",
    "model(l,r,dpt_map).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DCNv4 import modules as opsm\n",
    "deform_conv=getattr(opsm, 'DCNv4')\n",
    "class DCNConv(nn.Module):\n",
    "    \"\"\"\n",
    "    DCNv4\n",
    "    \"\"\"\n",
    "    def __init__(self,channels,kernel_size,group,offset_scale=1.0, output_bias=False):\n",
    "        super(DCNConv, self).__init__()\n",
    "\n",
    "        self.dcn_conv = deform_conv(\n",
    "                channels=channels,\n",
    "                group=group,\n",
    "                offset_scale=offset_scale,\n",
    "                dw_kernel_size=kernel_size,\n",
    "                output_bias=output_bias\n",
    "            )\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        _,_,H,W = inp.size()\n",
    "        inp = rearrange(inp, 'b c h w -> b (h w) c')\n",
    "        out = self.dcn_conv(inp,shape=[H,W])\n",
    "        out = rearrange(out, 'b (h w) c -> b c h w',h=H,w=W)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 192, 192])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "# print(torch.devices)\n",
    "dcn_test = DCNConv(64, 3, 4).to(device)\n",
    "img_in = torch.randn((2, 64, 192, 192)).to(device)\n",
    "dcn_test(img_in).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 190, 190])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(64, 64, kernel_size=3).to(device)\n",
    "conv(img_in).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
